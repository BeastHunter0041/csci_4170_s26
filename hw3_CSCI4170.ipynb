{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3dVazUz3/w2IvtCDeP5WV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeastHunter0041/csci_4170_s26/blob/main/hw3_CSCI4170.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1\n",
        "##Neural Network From Scratch (Regression Version)\n",
        "\n",
        "Goal:\n",
        "Predict **Number Captured** using a fully-connected neural network built from scratch.\n",
        "\n",
        "This implementation of a neural network includes includes:\n",
        "1. Data loading and preprocessing\n",
        "2. Train/test split\n",
        "3. Custom NeuralNetwork class\n",
        "4. Training with mini-batch gradient descent\n",
        "5. Evaluation (MSE and R²)\n"
      ],
      "metadata": {
        "id": "XSNkKxE_VDzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = \"/content/First_10100_GL_FishBiodiversity.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "IhgHvxsBVE8o",
        "outputId": "63128d23-6baf-4222-a475-b9faa96663a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10100, 93)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                    Project Name      Field Number       Date  Day  Month  \\\n",
              "0  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "1  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "2  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "3  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "4  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "\n",
              "   Year  Date fished   Waterbody Name WaterbodyType Arrival Time  ...  \\\n",
              "0  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "1  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "2  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "3  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "4  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "\n",
              "  Stream depth (m) depth_>_recorded Water velocity (msec) Bin Number  \\\n",
              "0              2.0            False                   NaN        0.0   \n",
              "1              2.0            False                   NaN        0.0   \n",
              "2              2.0            False                   NaN        0.0   \n",
              "3              2.0            False                   NaN        0.0   \n",
              "4              2.0            False                   NaN        0.0   \n",
              "\n",
              "   Bin_time_s                  Species  Number Captured  \\\n",
              "0         NaN    Micropterus nigricans              2.0   \n",
              "1         NaN        Lepomis peltastes              3.0   \n",
              "2         NaN  Notemigonus crysoleucas              2.0   \n",
              "3         NaN               Umbra limi              2.0   \n",
              "4         NaN     Notropis heterolepis              5.0   \n",
              "\n",
              "   Caught > Number captured Minimum (mm) Maximum (mm)  \n",
              "0                     False         0.00         0.00  \n",
              "1                     False        39.40        44.94  \n",
              "2                     False        32.60        68.70  \n",
              "3                     False        50.39        50.39  \n",
              "4                     False        22.89        46.10  \n",
              "\n",
              "[5 rows x 93 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-155e2076-eefa-41c7-a619-8ec0ae46c9c1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Project Name</th>\n",
              "      <th>Field Number</th>\n",
              "      <th>Date</th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "      <th>Date fished</th>\n",
              "      <th>Waterbody Name</th>\n",
              "      <th>WaterbodyType</th>\n",
              "      <th>Arrival Time</th>\n",
              "      <th>...</th>\n",
              "      <th>Stream depth (m)</th>\n",
              "      <th>depth_&gt;_recorded</th>\n",
              "      <th>Water velocity (msec)</th>\n",
              "      <th>Bin Number</th>\n",
              "      <th>Bin_time_s</th>\n",
              "      <th>Species</th>\n",
              "      <th>Number Captured</th>\n",
              "      <th>Caught &gt; Number captured</th>\n",
              "      <th>Minimum (mm)</th>\n",
              "      <th>Maximum (mm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Micropterus nigricans</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lepomis peltastes</td>\n",
              "      <td>3.0</td>\n",
              "      <td>False</td>\n",
              "      <td>39.40</td>\n",
              "      <td>44.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Notemigonus crysoleucas</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>32.60</td>\n",
              "      <td>68.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Umbra limi</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>50.39</td>\n",
              "      <td>50.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Notropis heterolepis</td>\n",
              "      <td>5.0</td>\n",
              "      <td>False</td>\n",
              "      <td>22.89</td>\n",
              "      <td>46.10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 93 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-155e2076-eefa-41c7-a619-8ec0ae46c9c1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-155e2076-eefa-41c7-a619-8ec0ae46c9c1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-155e2076-eefa-41c7-a619-8ec0ae46c9c1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Strategy\n",
        "\n",
        "- Target variable: **Number Captured**\n",
        "- Convert time columns to numeric minutes\n",
        "- One-hot encode small categorical columns\n",
        "- Standardize features (important for neural networks)\n",
        "- Drop rows with missing target\n"
      ],
      "metadata": {
        "id": "n9G2Ozu4VL3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"Number Captured\"\n",
        "\n",
        "data = df.dropna(subset=[TARGET]).copy()\n",
        "\n",
        "numeric_cols = [\n",
        "    \"Day\", \"Month\", \"Year\",\n",
        "    \"Start Latitude\", \"Start Longitude\",\n",
        "    \"Stop Latitude\", \"Stop Longitude\"\n",
        "]\n",
        "\n",
        "cat_cols = [\"WaterbodyType\", \"Gear\"]\n",
        "time_cols = [\"Start Time\", \"Stop Time\", \"Arrival Time\", \"Departure Time\"]\n",
        "\n",
        "use_cols = [c for c in numeric_cols + cat_cols + time_cols + [TARGET] if c in data.columns]\n",
        "data = data[use_cols].copy()\n",
        "\n",
        "def time_to_minutes(series):\n",
        "    t = pd.to_datetime(series, errors=\"coerce\")\n",
        "    return t.dt.hour * 60 + t.dt.minute\n",
        "\n",
        "for c in time_cols:\n",
        "    if c in data.columns:\n",
        "        data[c + \"_min\"] = time_to_minutes(data[c])\n",
        "\n",
        "if \"Start Time_min\" in data.columns and \"Stop Time_min\" in data.columns:\n",
        "    data[\"FishingDuration_min\"] = data[\"Stop Time_min\"] - data[\"Start Time_min\"]\n",
        "\n",
        "data = data.drop(columns=[c for c in time_cols if c in data.columns])\n",
        "\n",
        "cat_present = [c for c in cat_cols if c in data.columns]\n",
        "data = pd.get_dummies(data, columns=cat_present, dummy_na=True)\n",
        "\n",
        "# X\n",
        "X = data.drop(columns=[TARGET]).copy()\n",
        "X = X.fillna(0)\n",
        "\n",
        "X_np = X.to_numpy(dtype=np.float64)\n",
        "X_mu = X_np.mean(axis=0, keepdims=True)\n",
        "X_sigma = X_np.std(axis=0, keepdims=True) + 1e-8\n",
        "X_np = (X_np - X_mu) / X_sigma\n",
        "\n",
        "# y (log transform + standardize)\n",
        "y_raw = data[TARGET].astype(np.float64).to_numpy().reshape(-1, 1)\n",
        "y_log = np.log1p(y_raw)  # handles skew + huge counts safely\n",
        "\n",
        "y_mu = y_log.mean(axis=0, keepdims=True)\n",
        "y_sigma = y_log.std(axis=0, keepdims=True) + 1e-8\n",
        "y = (y_log - y_mu) / y_sigma\n",
        "\n",
        "def y_inverse_transform(y_pred_std):\n",
        "    \"\"\"\n",
        "    Convert standardized log-space predictions back to original 'Number captured' scale.\n",
        "    \"\"\"\n",
        "    y_pred_log = y_pred_std * y_sigma + y_mu\n",
        "    return np.expm1(y_pred_log)\n",
        "\n",
        "print(\"X shape:\", X_np.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"Raw y range:\", float(y_raw.min()), \"to\", float(y_raw.max()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ASK-gHoVMzV",
        "outputId": "02f9fec5-4d36-43fd-8686-f780810f1fd6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (9882, 30)\n",
            "y shape: (9882, 1)\n",
            "Raw y range: 0.0 to 2000.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1665434965.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  t = pd.to_datetime(series, errors=\"coerce\")\n",
            "/tmp/ipython-input-1665434965.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  t = pd.to_datetime(series, errors=\"coerce\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split_numpy(X, Y, test_size=0.2, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = X.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    test_n = int(n * test_size)\n",
        "    test_idx = idx[:test_n]\n",
        "    train_idx = idx[test_n:]\n",
        "    return X[train_idx], X[test_idx], Y[train_idx], Y[test_idx]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split_numpy(\n",
        "    X_np, y, test_size=0.2, seed=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Test:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjvAzwMYVRcc",
        "outputId": "76b40665-7df4-4194-dfbf-975e35b27db8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (7906, 30)\n",
            "Test: (1976, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Architecture\n",
        "\n",
        "- Fully connected layers\n",
        "- ReLU activation (hidden layers)\n",
        "- Linear output layer (regression)\n",
        "- Loss function: Mean Squared Error (MSE)\n",
        "- Optimization: Mini-batch Gradient Descent\n"
      ],
      "metadata": {
        "id": "lbjp3YCgVg8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(\n",
        "        self,\n",
        "        layer_dims,\n",
        "        learning_rate=0.001,\n",
        "        epochs=300,\n",
        "        batch_size=128,\n",
        "        seed=42,\n",
        "        print_every=25,\n",
        "        clip_value=1.0\n",
        "    ):\n",
        "        self.layer_dims = layer_dims\n",
        "        self.learning_rate = float(learning_rate)\n",
        "        self.epochs = int(epochs)\n",
        "        self.batch_size = int(batch_size) if batch_size is not None else None\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.print_every = int(print_every)\n",
        "        self.clip_value = float(clip_value)\n",
        "        self.params = self._init_params()\n",
        "\n",
        "    def _init_params(self):\n",
        "        params = {}\n",
        "        L = len(self.layer_dims) - 1\n",
        "        for l in range(1, L + 1):\n",
        "            fan_in = self.layer_dims[l - 1]\n",
        "            fan_out = self.layer_dims[l]\n",
        "            W = self.rng.normal(0, np.sqrt(2.0 / fan_in), size=(fan_in, fan_out))\n",
        "            b = np.zeros((1, fan_out))\n",
        "            params[f\"W{l}\"] = W\n",
        "            params[f\"b{l}\"] = b\n",
        "\n",
        "        # smaller output layer weights for stability\n",
        "        params[f\"W{L}\"] *= 0.1\n",
        "        return params\n",
        "\n",
        "    @staticmethod\n",
        "    def _relu(Z):\n",
        "        return np.maximum(0.0, Z)\n",
        "\n",
        "    @staticmethod\n",
        "    def _relu_backward(dA, Z):\n",
        "        dZ = dA.copy()\n",
        "        dZ[Z <= 0.0] = 0.0\n",
        "        return dZ\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          preds: (N, 1)\n",
        "          caches: list of dicts with A_prev, W, b, Z for each layer\n",
        "        \"\"\"\n",
        "        A = X\n",
        "        caches = []\n",
        "        L = len(self.layer_dims) - 1\n",
        "\n",
        "        # Hidden layers\n",
        "        for l in range(1, L):\n",
        "            W = self.params[f\"W{l}\"]\n",
        "            b = self.params[f\"b{l}\"]\n",
        "            A_prev = A\n",
        "            Z = A_prev @ W + b\n",
        "            A = self._relu(Z)\n",
        "            caches.append({\"A_prev\": A_prev, \"W\": W, \"b\": b, \"Z\": Z})\n",
        "\n",
        "        # Output layer (linear)\n",
        "        W = self.params[f\"W{L}\"]\n",
        "        b = self.params[f\"b{L}\"]\n",
        "        A_prev = A\n",
        "        ZL = A_prev @ W + b\n",
        "        caches.append({\"A_prev\": A_prev, \"W\": W, \"b\": b, \"Z\": ZL})\n",
        "\n",
        "        return ZL, caches\n",
        "\n",
        "    @staticmethod\n",
        "    def cost(preds, Y):\n",
        "        diff = preds - Y\n",
        "        return float(np.mean(diff * diff))\n",
        "\n",
        "    def backward(self, preds, Y, caches):\n",
        "        grads = {}\n",
        "        N = Y.shape[0]\n",
        "        L = len(self.layer_dims) - 1\n",
        "\n",
        "        # Output layer: MSE derivative\n",
        "        dZ = (2.0 / N) * (preds - Y)  # (N, 1)\n",
        "        out = caches[-1]\n",
        "        A_prev = out[\"A_prev\"]\n",
        "        W = out[\"W\"]\n",
        "\n",
        "        grads[f\"dW{L}\"] = A_prev.T @ dZ\n",
        "        grads[f\"db{L}\"] = np.sum(dZ, axis=0, keepdims=True)\n",
        "        dA_prev = dZ @ W.T\n",
        "\n",
        "        # Hidden layers\n",
        "        for l in range(L - 1, 0, -1):\n",
        "            cache = caches[l - 1]\n",
        "            A_prev = cache[\"A_prev\"]\n",
        "            W = cache[\"W\"]\n",
        "            Z = cache[\"Z\"]\n",
        "\n",
        "            dZ = self._relu_backward(dA_prev, Z)\n",
        "            grads[f\"dW{l}\"] = A_prev.T @ dZ\n",
        "            grads[f\"db{l}\"] = np.sum(dZ, axis=0, keepdims=True)\n",
        "            dA_prev = dZ @ W.T\n",
        "\n",
        "        # Gradient clipping\n",
        "        if self.clip_value is not None and self.clip_value > 0:\n",
        "            for k in grads:\n",
        "                np.clip(grads[k], -self.clip_value, self.clip_value, out=grads[k])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def _update_params(self, grads):\n",
        "        L = len(self.layer_dims) - 1\n",
        "        for l in range(1, L + 1):\n",
        "            self.params[f\"W{l}\"] -= self.learning_rate * grads[f\"dW{l}\"]\n",
        "            self.params[f\"b{l}\"] -= self.learning_rate * grads[f\"db{l}\"]\n",
        "\n",
        "    def _iterate_minibatches(self, X, Y):\n",
        "        N = X.shape[0]\n",
        "        idx = np.arange(N)\n",
        "        self.rng.shuffle(idx)\n",
        "        Xs, Ys = X[idx], Y[idx]\n",
        "\n",
        "        if self.batch_size is None:\n",
        "            yield Xs, Ys\n",
        "            return\n",
        "\n",
        "        for start in range(0, N, self.batch_size):\n",
        "            end = min(start + self.batch_size, N)\n",
        "            yield Xs[start:end], Ys[start:end]\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            losses = []\n",
        "            for Xb, Yb in self._iterate_minibatches(X, Y):\n",
        "                preds, caches = self.forward(Xb)\n",
        "\n",
        "                if not np.isfinite(preds).all():\n",
        "                    raise FloatingPointError(\"Predictions became NaN/Inf. Reduce LR or increase clipping.\")\n",
        "\n",
        "                loss = self.cost(preds, Yb)\n",
        "                if not np.isfinite(loss):\n",
        "                    raise FloatingPointError(\"Loss became NaN/Inf. Reduce LR or transform target.\")\n",
        "\n",
        "                grads = self.backward(preds, Yb, caches)\n",
        "                self._update_params(grads)\n",
        "                losses.append(loss)\n",
        "\n",
        "            if epoch % self.print_every == 0 or epoch == 1:\n",
        "                print(f\"Epoch {epoch}/{self.epochs} | MSE: {np.mean(losses):.6f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds, _ = self.forward(X)\n",
        "        return preds\n"
      ],
      "metadata": {
        "id": "Bdg0vna3Vibt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = X_train.shape[1]\n",
        "\n",
        "nn = NeuralNetwork(\n",
        "    layer_dims=[D, 64, 32, 1],\n",
        "    learning_rate=0.001,\n",
        "    epochs=300,\n",
        "    batch_size=128,\n",
        "    clip_value=1.0,\n",
        "    print_every=25\n",
        ")\n",
        "\n",
        "nn.train(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJrZBLnCVmaf",
        "outputId": "7939e7a0-0747-452e-cb26-c6643ed25c74"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300 | MSE: 1.020958\n",
            "Epoch 25/300 | MSE: 0.961700\n",
            "Epoch 50/300 | MSE: 0.942978\n",
            "Epoch 75/300 | MSE: 0.933704\n",
            "Epoch 100/300 | MSE: 0.927521\n",
            "Epoch 125/300 | MSE: 0.921191\n",
            "Epoch 150/300 | MSE: 0.916408\n",
            "Epoch 175/300 | MSE: 0.914859\n",
            "Epoch 200/300 | MSE: 0.911537\n",
            "Epoch 225/300 | MSE: 0.909935\n",
            "Epoch 250/300 | MSE: 0.908422\n",
            "Epoch 275/300 | MSE: 0.907145\n",
            "Epoch 300/300 | MSE: 0.905059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_train_std = nn.predict(X_train)\n",
        "preds_test_std  = nn.predict(X_test)\n",
        "\n",
        "# Metrics in standardized-log space\n",
        "train_mse_std = np.mean((preds_train_std - y_train) ** 2)\n",
        "test_mse_std  = np.mean((preds_test_std - y_test) ** 2)\n",
        "\n",
        "def r2_score(y_true, y_pred):\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return 1 - ss_res / ss_tot\n",
        "\n",
        "print(\"Standardized-log space:\")\n",
        "print(\"Train MSE:\", train_mse_std)\n",
        "print(\"Test MSE: \", test_mse_std)\n",
        "print(\"Train R²:\", r2_score(y_train, preds_train_std))\n",
        "print(\"Test R²: \", r2_score(y_test, preds_test_std))\n",
        "\n",
        "# Convert predictions back to original scale\n",
        "preds_train = y_inverse_transform(preds_train_std)\n",
        "preds_test  = y_inverse_transform(preds_test_std)\n",
        "\n",
        "y_train_raw = y_inverse_transform(y_train)\n",
        "y_test_raw  = y_inverse_transform(y_test)\n",
        "\n",
        "train_mse_raw = np.mean((preds_train - y_train_raw) ** 2)\n",
        "test_mse_raw  = np.mean((preds_test - y_test_raw) ** 2)\n",
        "\n",
        "print(\"\\nOriginal target space (Number captured):\")\n",
        "print(\"Train MSE:\", train_mse_raw)\n",
        "print(\"Test MSE: \", test_mse_raw)\n",
        "print(\"Train R²:\", r2_score(y_train_raw, preds_train))\n",
        "print(\"Test R²: \", r2_score(y_test_raw, preds_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubr3EQp4Vsh-",
        "outputId": "f7cddec8-457f-4db7-90db-f6c86b6b5e8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized-log space:\n",
            "Train MSE: 0.9041127572470533\n",
            "Test MSE:  0.8924624655942383\n",
            "Train R²: 0.10647310585284087\n",
            "Test R²:  0.06271221141960182\n",
            "\n",
            "Original target space (Number captured):\n",
            "Train MSE: 8504.997019676004\n",
            "Test MSE:  1691.218985305651\n",
            "Train R²: -0.005177487754313814\n",
            "Test R²:  -0.016277237487483998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Mini-Batch Gradient Descent?\n",
        "\n",
        "Full-batch:\n",
        "- Stable but slow for large datasets.\n",
        "\n",
        "Stochastic:\n",
        "- Very noisy updates.\n",
        "\n",
        "Mini-batch:\n",
        "- Efficient and stable.\n",
        "- Good tradeoff between runtime and convergence.\n",
        "- Works well in NumPy implementation.\n",
        "\n",
        "For this dataset, mini-batch significantly improves runtime compared to full-batch training.\n"
      ],
      "metadata": {
        "id": "cP2H5XrEVufp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Debugging + Performance Summary\n",
        "\n",
        "### What Went Wrong Initially\n",
        "- Target (Number captured) had large, skewed values --> exploding gradients --> NaN loss.\n",
        "- Learning rate was too high for regression.\n",
        "- Forward pass cached the wrong tensor (stored post-activation instead of A_prev) --> gradient shape mismatch --> broadcasting error.\n",
        "\n",
        "### Fixes Applied\n",
        "- I Applied log1p() transform + standardization to target.\n",
        "- Reduced learning rate (0.01 → 0.001).\n",
        "- I Added gradient clipping.\n",
        "- Corrected cache storage in forward pass.\n",
        "- Scaled down output layer initialization.\n",
        "\n",
        "---\n",
        "\n",
        "## Model Performance Analysis\n",
        "\n",
        "### Standardized-Log Space\n",
        "- Train MSE ≈ 0.90  \n",
        "- Test MSE ≈ 0.89  \n",
        "- Train R² ≈ 0.11  \n",
        "- Test R² ≈ 0.06  \n",
        "\n",
        "Interpretation:\n",
        "- Model is stable (no NaNs).\n",
        "- Small gap between train/test → no major overfitting.\n",
        "- Low R² indicates weak predictive signal in current feature set.\n",
        "\n",
        "### Original Target Space\n",
        "- Train MSE ≈ 8505  \n",
        "- Test MSE ≈ 1691  \n",
        "- Train R² ≈ -0.005  \n",
        "- Test R² ≈ -0.016  \n",
        "\n",
        "Interpretation:\n",
        "- Negative R² --> model performs roughly the same (or slightly worse) than predicting the mean.\n",
        "- Neural network is not extracting strong nonlinear structure.\n",
        "- Likely causes:\n",
        "  - Weak feature-target relationship.\n",
        "  - High noise in capture counts.\n",
        "  - Count data may require Poisson-style modeling instead of MSE regression.\n",
        "\n",
        "---\n",
        "\n",
        "### Overall Conclusion\n",
        "The model now trains correctly and generalizes consistently, but predictive power is low. Performance suggests either limited signal in the selected features or that a different modeling approach (e.g., Poisson regression, feature engineering, or interaction terms) may be more appropriate.\n"
      ],
      "metadata": {
        "id": "d5HEfb2tY9xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2"
      ],
      "metadata": {
        "id": "5WGbAaEbfOtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1 — PyTorch Resources Used (2-Layer Neural Network)\n",
        "\n",
        "Below are the PyTorch-only resources I used to learn the framework components needed to implement a 2-layer neural network and the required components for this homework.\n",
        "\n",
        "### 1) Defining the model + forward propagation (layers + forward())\n",
        "\n",
        "- **PyTorch Tutorial: “Build the Neural Network”**  \n",
        "  https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html  \n",
        "  **Why I needed it:** Shows how to create a model using torch.nn.Module, define layers like nn.Linear, and implement the forward() method. This directly maps to building a 2-layer NN and running the forward pass.\n",
        "\n",
        "\n",
        "\n",
        "### 2) Backward propagation via automatic differentiation (autograd)\n",
        "\n",
        "- **PyTorch Tutorial: “Autograd: Automatic Differentiation”**  \n",
        "  https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html  \n",
        "  **Why I needed it:** Explains how PyTorch builds a computation graph during the forward pass and computes gradients using loss.backward(). This replaces manual backprop from Part 1.\n",
        "\n",
        "- **API Reference: torch.autograd.backward**  \n",
        "  https://pytorch.org/docs/stable/generated/torch.autograd.backward.html  \n",
        "  **Why I needed it:** Clarifies what backward differentiation does, what tensors require gradients, and how gradients populate .grad -useful for debugging and correctness.\n",
        "\n",
        "\n",
        "\n",
        "### 3) Loss function (objective used for training)\n",
        "\n",
        "- **API Reference: torch.nn.MSELoss**  \n",
        "  https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html  \n",
        "  **Why I needed it:** My prediction target is numeric (**Number captured**), so this is a regression problem. `MSELoss` defines the training objective and documents expected input shapes/types.\n",
        "\n",
        "\n",
        "\n",
        "### 4) Gradient descent updates (optimizers)\n",
        "\n",
        "- **API Reference: torch.optim (optimizers overview)**  \n",
        "  https://pytorch.org/docs/stable/optim.html  \n",
        "  **Why I needed it:** Provides the standard training loop workflow for updating weights:\n",
        "  `optimizer.zero_grad()` --> `loss.backward()` --> `optimizer.step()`.\n",
        "\n",
        "- **API Reference: torch.optim.SGD**  \n",
        "  https://pytorch.org/docs/stable/generated/torch.optim.SGD.html  \n",
        "  **Why I needed it:** SGD is the closest framework equivalent to the gradient descent update implemented from scratch in Part 1.\n",
        "\n",
        "- **API Reference: torch.optim.Adam**  \n",
        "  https://pytorch.org/docs/stable/generated/torch.optim.Adam.html  \n",
        "  **Why I needed it:** Adam is a widely used optimizer that typically converges faster than vanilla SGD, making it useful for training stability and comparison.\n"
      ],
      "metadata": {
        "id": "4s06EoWLfKy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 — PyTorch 2-Layer Neural Network (Regression)\n",
        "\n",
        "Target: **Number Captured**\n",
        "\n",
        "Steps:\n",
        "1. EDA (cleaning + quick visualizations)\n",
        "2. Train / Dev / Test split\n",
        "3. Implement forward propagation (2-layer NN with activation)\n",
        "4. Compute cost (MSE)\n",
        "5. Train with gradient descent variant (Adam, mini-batch) + regularization\n",
        "   - Compare performance with normalized vs unnormalized inputs\n",
        "6. Report results on the test set\n"
      ],
      "metadata": {
        "id": "UuKpR6CWg5_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "DATA_PATH = \"/content/First_10100_GL_FishBiodiversity.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "xD4-r4Ofg8fX",
        "outputId": "5ef67525-d186-46da-bfcc-3a21d7b7666a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (10100, 93)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                    Project Name      Field Number       Date  Day  Month  \\\n",
              "0  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "1  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "2  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "3  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "4  Ausable Channel Sampling 2002  AUCR02-01-03-BEF  24-Sep-02   24      9   \n",
              "\n",
              "   Year  Date fished   Waterbody Name WaterbodyType Arrival Time  ...  \\\n",
              "0  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "1  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "2  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "3  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "4  2002          NaN  Ausable Channel        Stream          NaN  ...   \n",
              "\n",
              "  Stream depth (m) depth_>_recorded Water velocity (msec) Bin Number  \\\n",
              "0              2.0            False                   NaN        0.0   \n",
              "1              2.0            False                   NaN        0.0   \n",
              "2              2.0            False                   NaN        0.0   \n",
              "3              2.0            False                   NaN        0.0   \n",
              "4              2.0            False                   NaN        0.0   \n",
              "\n",
              "   Bin_time_s                  Species  Number Captured  \\\n",
              "0         NaN    Micropterus nigricans              2.0   \n",
              "1         NaN        Lepomis peltastes              3.0   \n",
              "2         NaN  Notemigonus crysoleucas              2.0   \n",
              "3         NaN               Umbra limi              2.0   \n",
              "4         NaN     Notropis heterolepis              5.0   \n",
              "\n",
              "   Caught > Number captured Minimum (mm) Maximum (mm)  \n",
              "0                     False         0.00         0.00  \n",
              "1                     False        39.40        44.94  \n",
              "2                     False        32.60        68.70  \n",
              "3                     False        50.39        50.39  \n",
              "4                     False        22.89        46.10  \n",
              "\n",
              "[5 rows x 93 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b201f346-b0e6-4322-a6ed-80c7fd783cc1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Project Name</th>\n",
              "      <th>Field Number</th>\n",
              "      <th>Date</th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "      <th>Date fished</th>\n",
              "      <th>Waterbody Name</th>\n",
              "      <th>WaterbodyType</th>\n",
              "      <th>Arrival Time</th>\n",
              "      <th>...</th>\n",
              "      <th>Stream depth (m)</th>\n",
              "      <th>depth_&gt;_recorded</th>\n",
              "      <th>Water velocity (msec)</th>\n",
              "      <th>Bin Number</th>\n",
              "      <th>Bin_time_s</th>\n",
              "      <th>Species</th>\n",
              "      <th>Number Captured</th>\n",
              "      <th>Caught &gt; Number captured</th>\n",
              "      <th>Minimum (mm)</th>\n",
              "      <th>Maximum (mm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Micropterus nigricans</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lepomis peltastes</td>\n",
              "      <td>3.0</td>\n",
              "      <td>False</td>\n",
              "      <td>39.40</td>\n",
              "      <td>44.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Notemigonus crysoleucas</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>32.60</td>\n",
              "      <td>68.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Umbra limi</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>50.39</td>\n",
              "      <td>50.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ausable Channel Sampling 2002</td>\n",
              "      <td>AUCR02-01-03-BEF</td>\n",
              "      <td>24-Sep-02</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ausable Channel</td>\n",
              "      <td>Stream</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Notropis heterolepis</td>\n",
              "      <td>5.0</td>\n",
              "      <td>False</td>\n",
              "      <td>22.89</td>\n",
              "      <td>46.10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 93 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b201f346-b0e6-4322-a6ed-80c7fd783cc1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b201f346-b0e6-4322-a6ed-80c7fd783cc1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b201f346-b0e6-4322-a6ed-80c7fd783cc1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Exploratory Data Analysis (EDA)\n",
        "\n",
        "We:\n",
        "- Inspect missingness and basic distributions\n",
        "- Focus on the target (`Number captured`)\n",
        "- Visualize target distribution (raw + log scale) since count targets are often skewed\n"
      ],
      "metadata": {
        "id": "NiDs-phvg-GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"Number Captured\"\n",
        "\n",
        "# Basic target checks\n",
        "print(\"Missing target:\", df[TARGET].isna().sum())\n",
        "print(df[TARGET].describe())\n",
        "\n",
        "# Plot raw distribution (may be skewed)\n",
        "plt.figure()\n",
        "df[TARGET].dropna().hist(bins=50)\n",
        "plt.title(\"Number captured (raw)\")\n",
        "plt.xlabel(\"count\")\n",
        "plt.ylabel(\"frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Plot log1p distribution (usually more stable)\n",
        "plt.figure()\n",
        "np.log1p(df[TARGET].dropna()).hist(bins=50)\n",
        "plt.title(\"log1p(Number captured)\")\n",
        "plt.xlabel(\"log1p(count)\")\n",
        "plt.ylabel(\"frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Quick missingness snapshot\n",
        "missing = df.isna().mean().sort_values(ascending=False).head(15)\n",
        "print(\"\\nTop missing columns (fraction missing):\")\n",
        "print(missing)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HVxqGkXRhARi",
        "outputId": "af207f73-cc1b-4cad-ff85-56ab29eeb8d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing target: 218\n",
            "count    9882.000000\n",
            "mean       15.934831\n",
            "std        84.294587\n",
            "min         0.000000\n",
            "25%         1.000000\n",
            "50%         3.000000\n",
            "75%         9.000000\n",
            "max      2000.000000\n",
            "Name: Number Captured, dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top missing columns (fraction missing):\n",
            "Date fished               1.0\n",
            "Unknown_aqua_veg          1.0\n",
            "Bin_time_s                1.0\n",
            "Salinity                  1.0\n",
            "Wind Speed (km/h)         1.0\n",
            "Dominant Vegetation       1.0\n",
            "Bank Slope (degrees)      1.0\n",
            "TDS (g/L)                 1.0\n",
            "Sample Area Length (m)    1.0\n",
            "Turbidity (ntu)           1.0\n",
            "Water velocity (msec)     1.0\n",
            "Not recorded              1.0\n",
            "None                      1.0\n",
            "Hardpan                   1.0\n",
            "Concrete                  1.0\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering / Cleaning\n",
        "\n",
        "We build a compact feature set:\n",
        "- Numeric: day/month/year, lat/long\n",
        "- Times converted to minutes (if present) + optional duration\n",
        "- Small categorical one-hot fields (e.g., WaterbodyType, Gear)\n",
        "\n",
        "Note: This is meant to be simple and reproducible; more feature engineering could improve R².\n"
      ],
      "metadata": {
        "id": "-W1K3Fs5hEHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def time_to_minutes(series: pd.Series) -> pd.Series:\n",
        "    t = pd.to_datetime(series, errors=\"coerce\")\n",
        "    return t.dt.hour * 60 + t.dt.minute\n",
        "\n",
        "# Drop rows missing target\n",
        "data = df.dropna(subset=[TARGET]).copy()\n",
        "\n",
        "numeric_cols = [\n",
        "    \"Day\", \"Month\", \"Year\",\n",
        "    \"Start Latitude\", \"Start Longitude\",\n",
        "    \"Stop Latitude\", \"Stop Longitude\"\n",
        "]\n",
        "cat_cols = [\"WaterbodyType\", \"Gear\"]\n",
        "time_cols = [\"Start Time\", \"Stop Time\", \"Arrival Time\", \"Departure Time\"]\n",
        "\n",
        "use_cols = [c for c in numeric_cols + cat_cols + time_cols + [TARGET] if c in data.columns]\n",
        "data = data[use_cols].copy()\n",
        "\n",
        "# Time features\n",
        "for c in time_cols:\n",
        "    if c in data.columns:\n",
        "        data[c + \"_min\"] = time_to_minutes(data[c])\n",
        "\n",
        "if \"Start Time_min\" in data.columns and \"Stop Time_min\" in data.columns:\n",
        "    data[\"FishingDuration_min\"] = data[\"Stop Time_min\"] - data[\"Start Time_min\"]\n",
        "\n",
        "# Drop raw time strings\n",
        "data = data.drop(columns=[c for c in time_cols if c in data.columns])\n",
        "\n",
        "# One-hot categoricals\n",
        "cat_present = [c for c in cat_cols if c in data.columns]\n",
        "data = pd.get_dummies(data, columns=cat_present, dummy_na=True)\n",
        "\n",
        "# X / y\n",
        "X_df = data.drop(columns=[TARGET]).copy()\n",
        "X_df = X_df.fillna(0)\n",
        "\n",
        "y_raw = data[TARGET].astype(np.float64).to_numpy().reshape(-1, 1)\n",
        "\n",
        "# Target transform (stability for count regression)\n",
        "y_log = np.log1p(y_raw)\n",
        "y_mu = y_log.mean(axis=0, keepdims=True)\n",
        "y_sigma = y_log.std(axis=0, keepdims=True) + 1e-8\n",
        "y_std = (y_log - y_mu) / y_sigma  # training target\n",
        "\n",
        "def y_inverse_transform(y_pred_std: np.ndarray) -> np.ndarray:\n",
        "    y_pred_log = y_pred_std * y_sigma + y_mu\n",
        "    return np.expm1(y_pred_log)\n",
        "\n",
        "X = X_df.to_numpy(dtype=np.float32)\n",
        "y = y_std.astype(np.float32)\n",
        "\n",
        "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n",
        "print(\"Raw target range:\", float(y_raw.min()), \"to\", float(y_raw.max()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIveFL1uhF6F",
        "outputId": "4255024a-ff2e-42b9-853b-d9ddf5467c8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (9882, 30) | y shape: (9882, 1)\n",
            "Raw target range: 0.0 to 2000.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3680336772.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  t = pd.to_datetime(series, errors=\"coerce\")\n",
            "/tmp/ipython-input-3680336772.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  t = pd.to_datetime(series, errors=\"coerce\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Train / Dev / Test Split\n",
        "\n",
        "We use:\n",
        "- Train: 70%\n",
        "- Dev: 15% (validation for tuning/early stopping decisions)\n",
        "- Test: 15% (final reporting only)\n"
      ],
      "metadata": {
        "id": "K2ME6zrihHxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dev_test_split(X, y, train=0.70, dev=0.15, test=0.15, seed=42):\n",
        "    assert abs(train + dev + test - 1.0) < 1e-9\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = X.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "\n",
        "    n_train = int(n * train)\n",
        "    n_dev = int(n * dev)\n",
        "\n",
        "    train_idx = idx[:n_train]\n",
        "    dev_idx = idx[n_train:n_train + n_dev]\n",
        "    test_idx = idx[n_train + n_dev:]\n",
        "\n",
        "    return (X[train_idx], y[train_idx],\n",
        "            X[dev_idx], y[dev_idx],\n",
        "            X[test_idx], y[test_idx])\n",
        "\n",
        "X_train_raw, y_train, X_dev_raw, y_dev, X_test_raw, y_test = train_dev_test_split(X, y, seed=42)\n",
        "\n",
        "print(\"Train:\", X_train_raw.shape, y_train.shape)\n",
        "print(\"Dev:  \", X_dev_raw.shape, y_dev.shape)\n",
        "print(\"Test: \", X_test_raw.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkRnnWGehI7r",
        "outputId": "c56c5ed9-3239-4f4b-9164-d36c5585a179"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (6917, 30) (6917, 1)\n",
            "Dev:   (1482, 30) (1482, 1)\n",
            "Test:  (1483, 30) (1483, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalized vs Unnormalized Inputs\n",
        "\n",
        "We will train the same 2-layer model twice:\n",
        "1) Using raw (unnormalized) input features  \n",
        "2) Using standardized (normalized) input features\n",
        "\n",
        "We compare Dev/Test performance to comment on normalization effects.\n"
      ],
      "metadata": {
        "id": "8xtEHbiNhKWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_fit(X_train):\n",
        "    mu = X_train.mean(axis=0, keepdims=True)\n",
        "    sigma = X_train.std(axis=0, keepdims=True) + 1e-8\n",
        "    return mu, sigma\n",
        "\n",
        "def standardize_apply(X, mu, sigma):\n",
        "    return (X - mu) / sigma\n",
        "\n",
        "# Fit normalizer on TRAIN only\n",
        "X_mu, X_sigma = standardize_fit(X_train_raw)\n",
        "\n",
        "X_train_norm = standardize_apply(X_train_raw, X_mu, X_sigma).astype(np.float32)\n",
        "X_dev_norm   = standardize_apply(X_dev_raw,   X_mu, X_sigma).astype(np.float32)\n",
        "X_test_norm  = standardize_apply(X_test_raw,  X_mu, X_sigma).astype(np.float32)\n"
      ],
      "metadata": {
        "id": "RYA8G268hLcu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Forward Propagation (2-layer NN)\n",
        "\n",
        "Architecture (2-layer):\n",
        "- Layer 1: Linear(input_dim → hidden_dim) + **ReLU**\n",
        "- Layer 2: Linear(hidden_dim → 1) (linear output for regression)\n",
        "\n",
        "Hyperparameters used:\n",
        "- hidden_dim: 64\n",
        "- activation: ReLU\n",
        "- optimizer: Adam (mini-batch)\n",
        "- batch_size: 128\n",
        "- epochs: 200\n",
        "- regularization: L2 via weight_decay = 1e-4\n",
        "- loss: MSELoss\n",
        "- device: CUDA (GPU) when available\n",
        "\n",
        "To evaluate the effect of feature scaling, the model was trained twice:\n",
        "1) using **unnormalized** inputs  \n",
        "2) using **standardized (normalized)** inputs\n"
      ],
      "metadata": {
        "id": "ckGqPF0LhM0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerRegressor(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)  # linear output\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "vixdO_37hOU3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4–5) Cost + Gradient Descent Training\n",
        "\n",
        "Cost:\n",
        "- Mean Squared Error (MSE): `nn.MSELoss()`\n",
        "\n",
        "Gradient descent variant:\n",
        "- Mini-batch Adam optimizer:\n",
        "  - `optimizer.zero_grad()`\n",
        "  - `loss.backward()`\n",
        "  - `optimizer.step()`\n",
        "\n",
        "Regularization:\n",
        "- L2 via `weight_decay` in Adam\n"
      ],
      "metadata": {
        "id": "nFyjcRtLhP04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loader(X, y, batch_size=128, shuffle=True):\n",
        "    X_t = torch.from_numpy(X)\n",
        "    y_t = torch.from_numpy(y)\n",
        "    ds = TensorDataset(X_t, y_t)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_mse(model, loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    mse_loss = nn.MSELoss()\n",
        "    losses = []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        preds = model(xb)\n",
        "        loss = mse_loss(preds, yb)\n",
        "        losses.append(loss.item())\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "def train_model(X_train, y_train, X_dev, y_dev,\n",
        "                hidden_dim=64, lr=1e-3, weight_decay=1e-4,\n",
        "                batch_size=128, epochs=200, device=\"cpu\"):\n",
        "    train_loader = make_loader(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
        "    dev_loader   = make_loader(X_dev,   y_dev,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = TwoLayerRegressor(input_dim=X_train.shape[1], hidden_dim=hidden_dim).to(device)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    history = {\"train_mse\": [], \"dev_mse\": []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            preds = model(xb)                 # forward propagation\n",
        "            loss = loss_fn(preds, yb)         # cost\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()                   # backward propagation via autograd\n",
        "            opt.step()                        # gradient descent update (Adam)\n",
        "\n",
        "        train_mse = eval_mse(model, train_loader, device=device)\n",
        "        dev_mse   = eval_mse(model, dev_loader,   device=device)\n",
        "        history[\"train_mse\"].append(train_mse)\n",
        "        history[\"dev_mse\"].append(dev_mse)\n",
        "\n",
        "        if epoch in (1, 10, 25, 50, 100, epochs):\n",
        "            print(f\"Epoch {epoch:3d}/{epochs} | train MSE={train_mse:.4f} | dev MSE={dev_mse:.4f}\")\n",
        "\n",
        "    return model, history\n"
      ],
      "metadata": {
        "id": "cPD-eUdRhSO2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Unnormalized training\n",
        "model_raw, hist_raw = train_model(\n",
        "    X_train_raw, y_train,\n",
        "    X_dev_raw,   y_dev,\n",
        "    hidden_dim=64,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    batch_size=128,\n",
        "    epochs=200,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Normalized training\n",
        "model_norm, hist_norm = train_model(\n",
        "    X_train_norm, y_train,\n",
        "    X_dev_norm,   y_dev,\n",
        "    hidden_dim=64,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    batch_size=128,\n",
        "    epochs=200,\n",
        "    device=device\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiOC65JjhTwJ",
        "outputId": "312dae0c-91a3-4057-dec3-8ecd9b1bb2c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Epoch   1/200 | train MSE=31.3530 | dev MSE=31.2226\n",
            "Epoch  10/200 | train MSE=2.0202 | dev MSE=2.0650\n",
            "Epoch  25/200 | train MSE=1.0891 | dev MSE=1.1873\n",
            "Epoch  50/200 | train MSE=1.0087 | dev MSE=1.1202\n",
            "Epoch 100/200 | train MSE=1.7142 | dev MSE=1.8334\n",
            "Epoch 200/200 | train MSE=1.0973 | dev MSE=1.1698\n",
            "Epoch   1/200 | train MSE=0.9868 | dev MSE=0.9888\n",
            "Epoch  10/200 | train MSE=0.8842 | dev MSE=0.9696\n",
            "Epoch  25/200 | train MSE=0.8641 | dev MSE=0.9631\n",
            "Epoch  50/200 | train MSE=0.8546 | dev MSE=0.9567\n",
            "Epoch 100/200 | train MSE=0.8571 | dev MSE=0.9565\n",
            "Epoch 200/200 | train MSE=0.8617 | dev MSE=0.9544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Test Set Results\n",
        "\n",
        "We report:\n",
        "- MSE and R² in the **standardized-log** training space (what the model optimizes)\n",
        "- MSE and R² converted back to the **original target scale** (\"Number captured\")\n"
      ],
      "metadata": {
        "id": "qrVmoBaShVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_numpy(model, X, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    X_t = torch.from_numpy(X).to(device)\n",
        "    preds = model(X_t).cpu().numpy()\n",
        "    return preds\n",
        "\n",
        "def r2_score_np(y_true, y_pred):\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return 1.0 - ss_res / ss_tot\n",
        "\n",
        "def report_results(name, model, X_test, y_test_std):\n",
        "    # Predictions in standardized-log space\n",
        "    yhat_std = predict_numpy(model, X_test, device=device)\n",
        "\n",
        "    mse_std = float(np.mean((yhat_std - y_test_std) ** 2))\n",
        "    r2_std  = float(r2_score_np(y_test_std, yhat_std))\n",
        "\n",
        "    # Convert to original target space\n",
        "    yhat_raw = y_inverse_transform(yhat_std)\n",
        "    ytest_raw = y_inverse_transform(y_test_std)\n",
        "\n",
        "    mse_raw = float(np.mean((yhat_raw - ytest_raw) ** 2))\n",
        "    r2_raw  = float(r2_score_np(ytest_raw, yhat_raw))\n",
        "\n",
        "    print(f\"\\n{name} — Test Results\")\n",
        "    print(\"Standardized-log space:\")\n",
        "    print(f\"  Test MSE: {mse_std:.4f}\")\n",
        "    print(f\"  Test R² : {r2_std:.4f}\")\n",
        "    print(\"Original target space (Number captured):\")\n",
        "    print(f\"  Test MSE: {mse_raw:.4f}\")\n",
        "    print(f\"  Test R² : {r2_raw:.4f}\")\n",
        "\n",
        "# Evaluate BOTH models on test\n",
        "report_results(\"UNNORMALIZED INPUTS\", model_raw,  X_test_raw,  y_test)\n",
        "report_results(\"NORMALIZED INPUTS\",   model_norm, X_test_norm, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osB_G_63hXNk",
        "outputId": "05c38bd8-95dc-4121-c4fe-f589612fff29"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNNORMALIZED INPUTS — Test Results\n",
            "Standardized-log space:\n",
            "  Test MSE: 1.2450\n",
            "  Test R² : -0.1444\n",
            "Original target space (Number captured):\n",
            "  Test MSE: 7631.0233\n",
            "  Test R² : -0.0242\n",
            "\n",
            "NORMALIZED INPUTS — Test Results\n",
            "Standardized-log space:\n",
            "  Test MSE: 0.9653\n",
            "  Test R² : 0.1127\n",
            "Original target space (Number captured):\n",
            "  Test MSE: 7551.7181\n",
            "  Test R² : -0.0136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization Commentary\n",
        "Normalization improved performance substantially in the standardized-log space:\n",
        "- Unnormalized test: MSE = 1.2450, R² = -0.1444\n",
        "- Normalized test:   MSE = 0.9653, R² = 0.1127\n",
        "\n",
        "This suggests standardizing inputs made optimization easier (more consistent feature scales) and improved generalization.\n",
        "\n",
        "In the original target scale (\"Number captured\"), performance remained weak:\n",
        "- Unnormalized test: MSE = 7631.0, R² = -0.0242\n",
        "- Normalized test:   MSE = 7551.7, R² = -0.0136\n",
        "\n",
        "R² staying slightly negative indicates the model is still roughly comparable to (or slightly worse than) predicting the mean in the raw count space. However, the normalized model clearly learns more effectively in the transformed (log-standardized) space, which is the space it is trained to optimize.\n"
      ],
      "metadata": {
        "id": "E5YdrHbLhYvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 — Hyperparameter Selection and Rationale\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "- **Hidden size (64):** Large enough to model nonlinear structure but small enough to avoid overfitting. Larger sizes did not improve validation performance.\n",
        "- **Learning rate (1e-3):** Standard Adam default; provided stable convergence without oscillation.\n",
        "- **Batch size (128):** Balanced gradient stability and computational efficiency.\n",
        "- **Epochs (200):** Training plateaued before 200 epochs, ensuring convergence.\n",
        "\n",
        "### Regularization\n",
        "\n",
        "Yes — L2 regularization (weight_decay = 1e-4) was used.  \n",
        "It helps prevent large weights and improves generalization on moderately sized, noisy data.\n",
        "\n",
        "### Optimization Algorithm\n",
        "\n",
        "Used **Adam optimizer** instead of vanilla SGD because:\n",
        "- It adapts learning rates automatically.\n",
        "- Converges faster and more reliably.\n",
        "- Requires less manual tuning.\n",
        "\n",
        "### Input Normalization\n",
        "\n",
        "Models trained with normalized inputs performed better on the dev/test sets.  \n",
        "Normalization improved optimization stability and generalization, which is consistent with standard neural network best practices.\n",
        "\n",
        "Overall, hyperparameters were selected based on validation performance, training stability, and simplicity.\n"
      ],
      "metadata": {
        "id": "vFPd8FXDo19n"
      }
    }
  ]
}